{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 연결\n",
    "\n",
    "### 브라우저로 정보를 가져오는 구조\n",
    "\n",
    "밥은 데스크톱 컴퓨터를 가지고 있고 앨리스의 서버에 연결하려고 함\n",
    "\n",
    "1. 밥의 컴퓨터는 1과 0으로 된 비트 스트림을 보냄. \n",
    "    - 각 비트는 전압으로 구별. \n",
    "    - 이들 비트는 정보를 구성하며, 헤더와 바디도 그런 정보에 포함\n",
    "    - 헤더에는 바로 다음 목표인 밥의 라우터 MAC 주소와 최종 목표인 앨리스 IP주소가 들어있음\n",
    "    - 바디에는 밥이 앨리스의 서버 애플리케이션에 요청하는 내용이 있음\n",
    "    \n",
    "2. 밥의 라우터는 이들 비트를 받아 인터넷에 보냄\n",
    "    - 밥의 MAC 주소에서 앨리스의 IP 주소로 가는 패킷을 해석\n",
    "    - 밥의 라우터의 고유 IP 주소를 패킷에 '발신자 from' 주소로 기록\n",
    "\n",
    "3. 밥의 패킷은 여러 중간 서버를 거치며 이동\n",
    "    - 중간 서버들은 정확한 물리적 경로 또는 유선 경로를 거쳐 앨리스의 서버를 향해 패킷을 보냄\n",
    "    \n",
    "4. 앨리스의 서버는 자신의 IP 주소에서 그 패킷을 받음\n",
    "\n",
    "5. 앨리스의 서버는 패킷 헤더에서 포트번호를 찾고 적절한 애플리케이션(웹 서버 애플리케이션)에 보냄\n",
    "    - 웹 애플리케이션의 포트는 거의 80\n",
    "    - IP 주소가 거리 주소라면 포트 번호는 패킷 데이터의 아파트 동수?\n",
    "    \n",
    "6. 웹 서버 애플리케이션은 서버 프로세서에서 데이터 스트림을 받음\n",
    "    - 다음과 같은 정보가 들어있음\n",
    "    - 이 요청은 GET 요청임\n",
    "    - 요청하는 파일은 index.html임\n",
    "    \n",
    "7. 웹 서버는 해당 HTML 파일을 찾고 새 패킷으로 묶어 자신의 라우터에 보냄\n",
    "    - 라우터는 밥의 컴퓨터로 전송\n",
    "    - 웹 서버가 보낸 패킷은 밥이 보낸 패킷과 같은 과정으로 밥의 컴퓨터에 도달\n",
    "    \n",
    "---\n",
    "#### 웹 브라우저는 이들 패킷을 만들고, 보내고, 돌아온 데이터를 해석해 표현하는 애플리케이션.\n",
    "##### 파이썬에서도 처리 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전송 받은 HTML 코드 : \n",
      "b'<html>\\n<head>\\n<title>A Useful Page</title>\\n</head>\\n<body>\\n<h1>An Interesting Title</h1>\\n<div>\\nLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\\n</div>\\n</body>\\n</html>\\n'\n"
     ]
    }
   ],
   "source": [
    "# urlopen은 네트워크를 통해 원격 객체를 읽음\n",
    "\n",
    "from urllib.request import urlopen\n",
    "html = urlopen(\"http://pythonscraping.com/pages/page1.html\")\n",
    "print(\"전송 받은 HTML 코드 : \")\n",
    "print(html.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1.2 BeautifulSoup\n",
    "잘못된 HTML을 수정하여 쉽게 탐색할 수 있는 XML 형식의 파이썬 객체로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "<title>A Useful Page</title>\n",
      "</head>\n",
      "<body>\n",
      "<h1>An Interesting Title</h1>\n",
      "<div>\n",
      "Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n",
      "</div>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "html = urlopen(\"http://pythonscraping.com/pages/page1.html\")\n",
    "bsObj = BeautifulSoup(html.read(), \"html.parser\")\n",
    "\n",
    "print(bsObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>An Interesting Title</h1>\n",
      "<h1>An Interesting Title</h1>\n",
      "<h1>An Interesting Title</h1>\n"
     ]
    }
   ],
   "source": [
    "# 셋 다 동일\n",
    "print(bsObj.html.body.h1)\n",
    "print(bsObj.body.h1)\n",
    "print(bsObj.h1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 신뢰할 수 있는 연결 (예외처리)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>An Interesting Title</h1>\n"
     ]
    }
   ],
   "source": [
    "from urllib.error import  HTTPError\n",
    "\n",
    "def getTitle(url):\n",
    "    try:\n",
    "        html = urlopen(url)\n",
    "    # 400, 500 error\n",
    "    except HTTPError as e:\n",
    "        return None\n",
    "    try:\n",
    "        bsObj = BeautifulSoup(html.read(), \"html.parser\")\n",
    "        title = bsObj.body.h1\n",
    "    # 반환된 객체가 없는 경우\n",
    "    except AttributeError as e:\n",
    "        return None\n",
    "    return title\n",
    "\n",
    "title = getTitle(\"http://www.pythonscraping.com/pages/page1.html\")\n",
    "if title == None:\n",
    "    print(\"Title could not be found!\")\n",
    "else:\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 스타일시트 사용\n",
    "* CSS의 등장은 웹 스크레이퍼에도 큰 도움이 됨\n",
    "* 클래스를 이용해 쉽게 태그들을 구별\n",
    "\n",
    "### .get_text()\n",
    "* 현재 문서에서 모든 태그를 제거하고 문자열만 반환\n",
    "* 텍스트 블록보다는 BeautifulSoup 객체를 사용하는게 더 유용\n",
    "* 그래서 최종 데이터를 출력하거나 저장, 조작 직전에만 사용\n",
    "\n",
    "### find()와 findAll()\n",
    "* find(tag, attributes, recusive, text, keywords)\n",
    "* findAll(tag, attributes, recusive, text, limit, keywords)\n",
    "* 실제로 두 매개변수(tag, attributes)만 거의 사용\n",
    "\n",
    "\n",
    "* tag :\n",
    "    * 태그 문자열 이름이나 리스트로 넘김\n",
    "    * `.findAll({\"h1\", \"h2\", \"h3\", \"h4\"})`\n",
    "* attributes : \n",
    "    * 속성으로 이루어진 파이썬 딕셔너리를 받고 그중 하나 일치하는 태그 찾음\n",
    "    * .findAll(\"span\",{\"class\":{\"green\",\"red\"}})\n",
    "* recursive :\n",
    "    * boolean 값임\n",
    "    * True면 일치하는 태그를 찾아 자식, 자식의 자식 검색\n",
    "    * false면 문서의 최상위 태그만 검색\n",
    "    * 재귀적으로 True로 설정돼있음\n",
    "* text :\n",
    "    * 태그 속성이 아니라 텍스트 콘텐츠에 일치\n",
    "    * `.findAll(text=\"the prince\")`\n",
    "* limit : \n",
    "    * findAll에서 limit를 1로 지정하면 find와 같음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna\n",
      "Pavlovna Scherer\n"
     ]
    }
   ],
   "source": [
    "html = urlopen(\"http://www.pythonscraping.com/pages/warandpeace.html\")\n",
    "bsObj = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "nameList1 = bsObj.findAll(\"span\",{\"class\":\"green\"},limit=1)\n",
    "for name in nameList1:\n",
    "    print(name.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna\n",
      "Pavlovna Scherer\n"
     ]
    }
   ],
   "source": [
    "nameList2 = bsObj.find(\"span\",{\"class\":\"green\"})\n",
    "print(nameList2.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "nameList3 = bsObj.findAll(text=\"the prince\")\n",
    "print(len(nameList3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 트리 이동\n",
    "* 문서 안에서의 위치를 기준으로 태그 검색\n",
    "* .children : 자식만 검색\n",
    "* .descendants : 자식에 포함된 태그(img, span 등등) 모두 포함되서 검색\n",
    "* .next_siblings, previous_siblings : 객체 자신을 제외한 형제들을 반환\n",
    "* .next_sibling, previous_sibling : 태그 하나만 반환\n",
    "* .parent : 부모 검색\n",
    "* 페이지 레이아웃은 시시때때 변함. 따라서 가능하다면 태그 속성을 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "$15.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "html = urlopen(\"http://www.pythonscraping.com/pages/page3.html\")\n",
    "bsObj = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "print(bsObj.find(\"img\",{\"src\":\"../img/gifts/img1.jpg\"}).parent.previous_sibling.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정규 표현식\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<img src=\"../img/gifts/img1.jpg\"/>\n",
      "<img src=\"../img/gifts/img2.jpg\"/>\n",
      "<img src=\"../img/gifts/img3.jpg\"/>\n",
      "<img src=\"../img/gifts/img4.jpg\"/>\n",
      "<img src=\"../img/gifts/img6.jpg\"/>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "html = urlopen(\"http://www.pythonscraping.com/pages/page3.html\")\n",
    "bsObj = BeautifulSoup(html, \"html.parser\")\n",
    "images = bsObj.findAll(\"img\", {\"src\": re.compile(\"\\.\\.\\/img\\/gifts\\/img.*\\.jpg\")})\n",
    "\n",
    "for image in images:\n",
    "    print(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 속성에 접근하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../img/gifts/img1.jpg\n",
      "../img/gifts/img2.jpg\n",
      "../img/gifts/img3.jpg\n",
      "../img/gifts/img4.jpg\n",
      "../img/gifts/img6.jpg\n"
     ]
    }
   ],
   "source": [
    "for image in images:\n",
    "    print(image[\"src\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 람다 표현식\n",
    "* 간단히 말해 다른 함수에 변수로 전달되는 함수\n",
    "* 태그 객체를 매개변수로 받아야 함\n",
    "* boolean만 반환 할 수 있음\n",
    "* BeautifulSoup는 모든 객체를 이 함수에서 평가하고, true로 평가된 태그만 반환\n",
    "* `soup.findAll(lambda tag:len(tag.attrs) == 2)`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
